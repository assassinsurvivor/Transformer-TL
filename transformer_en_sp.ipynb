{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_en_sp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahPzpWixJXt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e0348a1-c4d9-455d-803a-db0c514824d6"
      },
      "source": [
        "% tensorflow_version 2.x\n",
        "\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gsbVjxwLsWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IEVabNPaBSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.rstrip().strip()\n",
        "  return w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIsjIDVQagYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s1K4nfhaqFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "en=list(en)\n",
        "sp=list(sp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnU6KcT3cGuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_en=tfds.features.text.SubwordTextEncoder.build_from_corpus((i for i in en),target_vocab_size=8000)\n",
        "token_sp=tfds.features.text.SubwordTextEncoder.build_from_corpus((i for i in sp),target_vocab_size=8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZeTM5QGGrdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2054df9-612f-4893-996f-15a44c4576d8"
      },
      "source": [
        "token_en.vocab_size,token_sp.vocab_size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8148, 7902)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIkp-wDzdEM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx,i in enumerate(en):\n",
        "  en[idx]=[token_en.vocab_size]+token_en.encode(i)+[token_en.vocab_size+1]\n",
        "\n",
        "for idx,i in enumerate(sp):\n",
        "  sp[idx]=[token_sp.vocab_size]+token_sp.encode(i)+[token_sp.vocab_size+1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTTgMPAvbVgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(en,sp,test_size=0.3,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa4ciCuyauhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data():\n",
        "  for _ in range(len(x_train)):\n",
        "    yield x_train[_],y_train[_]\n",
        "\n",
        "def generate_valid_data():\n",
        "  for _ in range(len(x_test)):\n",
        "    yield x_test[_],y_test[_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vya6DflBa1LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=300\n",
        "buffer=30000\n",
        "dataset=tf.data.Dataset.from_generator(generate_data,output_types=(tf.int32,tf.int32))\n",
        "x_t=dataset.cache().shuffle(buffer).padded_batch(batch_size,padded_shapes=([-1], [-1])).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_dataset=tf.data.Dataset.from_generator(generate_valid_data,output_types=(tf.int32,tf.int32))\n",
        "x_v=valid_dataset.cache().shuffle(buffer).padded_batch(batch_size,padded_shapes=([-1], [-1])).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cAWcqYJepat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nmt_transformer import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA6GpjHkiE_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim=600\n",
        "num_coders=6\n",
        "num_attn_head=4\n",
        "higher_dim=1024\n",
        "input_vocab_size=token_en.vocab_size+2\n",
        "target_vocab_size=token_sp.vocab_size+2\n",
        "max_length_input=500\n",
        "max_length_target=500\n",
        "\n",
        "\n",
        "transformer=Transformer(num_coders=num_coders,num_attn_head=num_attn_head,embed_dim=embed_dim,higher_dim=higher_dim,\\\n",
        "                        input_vocab_size=input_vocab_size,target_vocab_size=target_vocab_size\\\n",
        "                        ,max_length_input=max_length_input,max_length_target=max_length_target,drop_rate=0.05)\n",
        "\n",
        "lr=LearningRate(embed_dim,warmup_steps=9000)\n",
        "optimizer=tf.keras.optimizers.Adam(lr,epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djriPZpDlOoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating train  graph\n",
        "inp_signature=[tf.TensorSpec(shape=(None,None),dtype=tf.int32),tf.TensorSpec(shape=(None,None),dtype=tf.int32),tf.TensorSpec(shape=(None),dtype=tf.bool)]\n",
        "@tf.function(input_signature=inp_signature)\n",
        "def train(inp,target,flag):\n",
        "    decoder_input=target[:,:-1] #example for output '<start> I am a student <eos>' the values that should be fed to the decoder should be '<start> I am a'\n",
        "    decoder_output=target[:,1:] # 'I am a student <eos>'\n",
        "    encoder_mask,decoder_look_ahead=create_mask(inp,decoder_input)\n",
        "    with tf.GradientTape() as tape:\n",
        "        output,attn_dict,encoder_op=transformer(inp,decoder_input,encoder_mask,decoder_look_ahead,True)\n",
        "        if flag==True:\n",
        "            print(transformer.summary())\n",
        "            flag=False\n",
        "        loss_=calculate_loss(decoder_output,output)\n",
        "    \n",
        "    gradients=tape.gradient(loss_,transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))\n",
        "    \n",
        "    train_loss.update_state(loss_)\n",
        "    train_accuracy.update_state(decoder_output,output)\n",
        "\n",
        "\n",
        "#creating validation graph\n",
        "inp_signature=[tf.TensorSpec(shape=(None,None),dtype=tf.int32),tf.TensorSpec(shape=(None,None),dtype=tf.int32)]\n",
        "@tf.function(input_signature=inp_signature)\n",
        "def validate(inp,target):\n",
        "    decoder_input=target[:,:-1] #example for output '<start> I am a student <eos>' the values that should be fed to the decoder should be '<start> I am a'\n",
        "    decoder_output=target[:,1:] # 'I am a student <eos>'\n",
        "    encoder_mask,decoder_look_ahead=create_mask(inp,decoder_input)\n",
        "    output,attn_dict,encoder_op=transformer(inp,decoder_input,encoder_mask,decoder_look_ahead,True)\n",
        "    loss_valid=calculate_loss(decoder_output,output)\n",
        "    valid_loss.update_state(loss_valid)\n",
        "    valid_accuracy.update_state(decoder_output,output)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv_auT9ejVYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"./en_sp/train/\"\n",
        "ckpt=tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
        "ckpt_manage=tf.train.CheckpointManager(checkpoint=ckpt,directory=path,max_to_keep=4)\n",
        "if ckpt_manage.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manage.latest_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIhnIXFnjend",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "450170a5-2432-4d67-b8da-f890e70421b0"
      },
      "source": [
        "epochs=3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss.reset_states();train_accuracy.reset_states();valid_loss.reset_states();valid_accuracy.reset_states()\n",
        "    \n",
        "    for (batch,(inp,target)) in enumerate(x_t):\n",
        "        \n",
        "        \n",
        "        if batch ==0 and epoch ==0:\n",
        "          with tf.device(\"/gpu:0\"):\n",
        "            train(inp,target,True)\n",
        "        else:\n",
        "          with tf.device(\"/gpu:0\"):\n",
        "            train(inp,target,False)\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "              print (' For epoch {} with  batch {}  the loss is  {:.4f}  with accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        ckpt_save = ckpt_manage.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save))\n",
        "\n",
        "    \n",
        "    for (valid_batch,(inp,target)) in enumerate(x_v):\n",
        "      with tf.device(\"/gpu:0\"):\n",
        "        validate(inp,target)\n",
        "    \n",
        "    print ('For epoch {} the validation loss is {:.4f} with accuracy of {:.4f}'.format(epoch + 1, \n",
        "                                                valid_loss.result(), \n",
        "                                                valid_accuracy.result()))\n",
        "    \n",
        "\n",
        "    \n",
        "    print ('For epoch {} the loss is {:.4f} with accuracy of {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_nx (encoder_nx)      multiple                  20941344  \n",
            "_________________________________________________________________\n",
            "decoder_nx (decoder_nx)      multiple                  29455344  \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             multiple                  4750304   \n",
            "=================================================================\n",
            "Total params: 55,146,992\n",
            "Trainable params: 55,146,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_nx (encoder_nx)      multiple                  20941344  \n",
            "_________________________________________________________________\n",
            "decoder_nx (decoder_nx)      multiple                  29455344  \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             multiple                  4750304   \n",
            "=================================================================\n",
            "Total params: 55,146,992\n",
            "Trainable params: 55,146,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            " For epoch 1 with  batch 0  the loss is  0.7830  with accuracy 0.1514\n",
            " For epoch 1 with  batch 100  the loss is  0.7122  with accuracy 0.2032\n",
            " For epoch 1 with  batch 200  the loss is  0.6725  with accuracy 0.2050\n",
            "Saving checkpoint for epoch 1 at ./en_sp/train/ckpt-6\n",
            "For epoch 1 the validation loss is 0.7924 with accuracy of 0.1927\n",
            "For epoch 1 the loss is 0.6529 with accuracy of 0.2037\n",
            " For epoch 2 with  batch 0  the loss is  0.6124  with accuracy 0.2259\n",
            " For epoch 2 with  batch 100  the loss is  0.6066  with accuracy 0.2125\n",
            " For epoch 2 with  batch 200  the loss is  0.5973  with accuracy 0.2149\n",
            "Saving checkpoint for epoch 2 at ./en_sp/train/ckpt-7\n",
            "For epoch 2 the validation loss is 0.7737 with accuracy of 0.1971\n",
            "For epoch 2 the loss is 0.5909 with accuracy of 0.2133\n",
            " For epoch 3 with  batch 0  the loss is  0.5501  with accuracy 0.2120\n",
            " For epoch 3 with  batch 100  the loss is  0.5459  with accuracy 0.2168\n",
            " For epoch 3 with  batch 200  the loss is  0.5505  with accuracy 0.2186\n",
            "Saving checkpoint for epoch 3 at ./en_sp/train/ckpt-8\n",
            "For epoch 3 the validation loss is 0.7547 with accuracy of 0.2002\n",
            "For epoch 3 the loss is 0.5508 with accuracy of 0.2193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzB-PnwoQ1nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_en.save_to_file(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGuC4OOCQ5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_sp.save_to_file(\"sp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQTM8HLQQ70s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}